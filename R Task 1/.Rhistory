n_samples <- 10000  # Number of samples to draw
threshold <- 1  # Define a threshold for the acceptable difference between simulated and observed data
# Define a function for the model prediction (using Model 5's formula)
predict_model_5 <- function(theta_1, theta_2, x1, x3, x4, x5) {
return(theta_1 * x1 + theta_2 * I(x4^2) + model_5_coeffs["x3"] * x3 + model_5_coeffs["I(x5^3)"] * I(x5^3))
}
# Store accepted samples
accepted_samples <- data.frame(theta_1 = numeric(0), theta_2 = numeric(0))
# Perform rejection ABC
for (i in 1:n_samples) {
# Draw a sample from the uniform prior
theta_1_sample <- runif(1, prior_range_1[1], prior_range_1[2])
theta_2_sample <- runif(1, prior_range_2[1], prior_range_2[2])
# Simulate the model with the sampled parameters
y_simulated <- predict_model_5(theta_1_sample, theta_2_sample, data$x1, data$x3, data$x4, data$x5)
# Calculate the discrepancy (e.g., sum of squared residuals between simulated and actual data)
discrepancy <- sum((y_simulated - data$x2)^2)
# Accept the sample if the discrepancy is below the threshold
if (discrepancy < threshold) {
accepted_samples <- rbind(accepted_samples, data.frame(theta_1 = theta_1_sample, theta_2 = theta_2_sample))
}
}
# Joint posterior distribution
ggplot(accepted_samples, aes(x = theta_1, y = theta_2)) +
geom_point(alpha = 0.5) +
labs(title = "Joint Posterior Distribution of theta_1 and theta_2", x = "theta_1", y = "theta_2")
# Marginal posterior distributions
ggplot(accepted_samples, aes(x = theta_1)) +
geom_histogram(bins = 50, fill = "blue", alpha = 0.7) +
labs(title = "Marginal Posterior Distribution of theta_1", x = "theta_1", y = "Density")
ggplot(accepted_samples, aes(x = theta_2)) +
geom_histogram(bins = 50, fill = "red", alpha = 0.7) +
labs(title = "Marginal Posterior Distribution of theta_2", x = "theta_2", y = "Density")
# Show the results
cat("Number of accepted samples:", nrow(accepted_samples), "\n")
cat("Accepted parameter samples for theta_1 and theta_2:\n")
print(head(accepted_samples))
# Load necessary libraries
library(ggplot2)
data <- read.csv("E:/R Task/R Task 3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Using coefficients from Model 5 as the selected model
model_5_coeffs <- coef(model_5)
# Extract the two parameters with the largest absolute value from Model 5
# For this example, we assume the two largest parameters are theta_1 and theta_2
theta_1_est <- model_5_coeffs["x1"]
theta_2_est <- model_5_coeffs["I(x4^2)"]
# Set up the prior distributions around the estimated values
# Let's assume we use a uniform distribution around the estimated values
prior_range_1 <- c(theta_1_est - 0.1, theta_1_est + 0.1)
prior_range_2 <- c(theta_2_est - 0.1, theta_2_est + 0.1)
# Rejection ABC parameters
n_samples <- 10000  # Number of samples to draw
threshold <- 1  # Define a threshold for the acceptable difference between simulated and observed data
# Define a function for the model prediction (using Model 5's formula)
predict_model_5 <- function(theta_1, theta_2, x1, x3, x4, x5) {
return(theta_1 * x1 + theta_2 * I(x4^2) + model_5_coeffs["x3"] * x3 + model_5_coeffs["I(x5^3)"] * I(x5^3))
}
# Store accepted samples
accepted_samples <- data.frame(theta_1 = numeric(0), theta_2 = numeric(0))
# Perform rejection ABC
for (i in 1:n_samples) {
# Draw a sample from the uniform prior
theta_1_sample <- runif(1, prior_range_1[1], prior_range_1[2])
theta_2_sample <- runif(1, prior_range_2[1], prior_range_2[2])
# Simulate the model with the sampled parameters
y_simulated <- predict_model_5(theta_1_sample, theta_2_sample, data$x1, data$x3, data$x4, data$x5)
# Calculate the discrepancy (e.g., sum of squared residuals between simulated and actual data)
discrepancy <- sum((y_simulated - data$x2)^2)
# Accept the sample if the discrepancy is below the threshold
if (discrepancy < threshold) {
accepted_samples <- rbind(accepted_samples, data.frame(theta_1 = theta_1_sample, theta_2 = theta_2_sample))
}
}
# Save the plots to a PDF file
pdf("ABC_Posteriors_Plots.pdf", width = 12, height = 8)
# Joint posterior distribution
ggplot(accepted_samples, aes(x = theta_1, y = theta_2)) +
geom_point(alpha = 0.5) +
labs(title = "Joint Posterior Distribution of theta_1 and theta_2", x = "theta_1", y = "theta_2")
# Marginal posterior distributions for theta_1 and theta_2
ggplot(accepted_samples, aes(x = theta_1)) +
geom_histogram(bins = 50, fill = "blue", alpha = 0.7) +
labs(title = "Marginal Posterior Distribution of theta_1", x = "theta_1", y = "Density")
ggplot(accepted_samples, aes(x = theta_2)) +
geom_histogram(bins = 50, fill = "red", alpha = 0.7) +
labs(title = "Marginal Posterior Distribution of theta_2", x = "theta_2", y = "Density")
# Close the PDF device
dev.off()
# Show the results
cat("Number of accepted samples:", nrow(accepted_samples), "\n")
cat("Accepted parameter samples for theta_1 and theta_2:\n")
print(head(accepted_samples))
# Load necessary libraries
library(MASS)
library(ggplot2)
# Step 1: Load the dataset
data <- read.csv("E:/R Task/R Task 3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Define input (X) and output (y) variables
X <- data[, c("x1", "x3", "x4", "x5")]
y <- data$x2  # Output variable
# Fit Model 1: Linear Model y = θ0 + θ1*x1 + θ2*x3
model_1 <- lm(y ~ x1 + x3, data = data)
cat("Model 1 Coefficients:\n")
print(coef(model_1))
# Fit Model 2: Polynomial Model y = θ0 + θ1*x1^2 + θ2*x3^2
model_2 <- lm(y ~ I(x1^2) + I(x3^2), data = data)
cat("Model 2 Coefficients:\n")
print(coef(model_2))
# Fit Model 3: Polynomial Model y = θ0 + θ1*x1^3 + θ2*x3^3
model_3 <- lm(y ~ I(x1^3) + I(x3^3), data = data)
cat("Model 3 Coefficients:\n")
print(coef(model_3))
# Fit Model 4: Polynomial Model y = θ0 + θ1*x1 + θ2*x3 + θ3*x4^2
model_4 <- lm(y ~ x1 + x3 + I(x4^2), data = data)
cat("Model 4 Coefficients:\n")
print(coef(model_4))
# Fit Model 5: Polynomial Model y = θ0 + θ1*x1 + θ2*x3 + θ3*x4^2 + θ4*x5^3
model_5 <- lm(y ~ x1 + x3 + I(x4^2) + I(x5^3), data = data)
cat("Model 5 Coefficients:\n")
print(coef(model_5))
# Residual Sum of Squares (RSS) for each model
RSS_1 <- sum(residuals(model_1)^2)
RSS_2 <- sum(residuals(model_2)^2)
RSS_3 <- sum(residuals(model_3)^2)
RSS_4 <- sum(residuals(model_4)^2)
RSS_5 <- sum(residuals(model_5)^2)
cat("RSS for Model 1:", RSS_1, "\n")
cat("RSS for Model 2:", RSS_2, "\n")
cat("RSS for Model 3:", RSS_3, "\n")
cat("RSS for Model 4:", RSS_4, "\n")
cat("RSS for Model 5:", RSS_5, "\n")
# Assuming Model 1 is selected based on the smallest RSS
model_selected <- model_1  # Change this based on task requirements
# Extract coefficients
theta_hat <- coef(model_selected)
# Print the coefficients
cat("Estimated coefficients for the selected model:\n")
print(theta_hat)
# Select two parameters with the largest absolute value from coefficients
param_values <- abs(theta_hat[-1])  # Exclude intercept
param_names <- names(theta_hat)[-1]  # Exclude intercept
largest_params <- sort(param_values, decreasing = TRUE)[1:2]
selected_params <- param_names[param_values %in% largest_params]
selected_theta_hat <- theta_hat[selected_params]
cat("Selected Parameters and their Estimates:\n")
print(selected_theta_hat)
# Step 1: Define uniform prior ranges
# Use a range of ±10% of the estimated coefficients as prior
prior_ranges <- sapply(selected_theta_hat, function(x) abs(x) * 0.1)
# Step 2: Draw samples from the uniform prior
n_samples <- 10000
samples <- matrix(NA, nrow = n_samples, ncol = length(selected_params))
colnames(samples) <- selected_params
set.seed(123)  # For reproducibility
for (i in 1:n_samples) {
samples[i, ] <- runif(length(selected_params),
min = selected_theta_hat - prior_ranges,
max = selected_theta_hat + prior_ranges)
}
# Step 3: Rejection ABC - Define the distance metric (squared error between predicted and observed values)
abc_distance <- function(sample, model, X, y) {
model$coefficients[selected_params] <- sample
predictions <- predict(model, newdata = X)
sum((predictions - y)^2)
}
# Step 4: Perform Rejection ABC
accepted_samples <- list()
threshold <- 1000  # Acceptable distance threshold (you may adjust based on the dataset)
for (i in 1:n_samples) {
sample <- samples[i, ]
dist <- abc_distance(sample, model_selected, X, y)
if (dist < threshold) {
accepted_samples <- append(accepted_samples, list(sample))
}
}
# Convert accepted samples to a data frame
accepted_samples_df <- do.call(rbind, accepted_samples)
colnames(accepted_samples_df) <- selected_params
# Joint posterior distribution
ggplot(data = as.data.frame(accepted_samples_df), aes(x = selected_params[1], y = selected_params[2])) +
geom_point(alpha = 0.5) +
ggtitle("Joint Posterior Distribution for Parameters") +
xlab(selected_params[1]) + ylab(selected_params[2])
# Marginal posterior distribution for the first parameter
ggplot(data = as.data.frame(accepted_samples_df), aes_string(x = selected_params[1])) +
geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black", alpha = 0.7) +
ggtitle(paste("Marginal Posterior Distribution for", selected_params[1])) +
xlab(selected_params[1]) + ylab("Frequency")
# Marginal posterior distribution for the second parameter
ggplot(data = as.data.frame(accepted_samples_df), aes_string(x = selected_params[2])) +
geom_histogram(binwidth = 0.05, fill = "salmon", color = "black", alpha = 0.7) +
ggtitle(paste("Marginal Posterior Distribution for", selected_params[2])) +
xlab(selected_params[2]) + ylab("Frequency")
# Load necessary libraries
library(MASS)
library(ggplot2)
# Step 1: Load the dataset
data <- read.csv("E:/R Task/R Task 3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Define input (X) and output (y) variables
X <- data[, c("x1", "x3", "x4", "x5")]
y <- data$x2  # Output variable
# Fit Model 1: Linear Model y = θ0 + θ1*x1 + θ2*x3
model_1 <- lm(y ~ x1 + x3, data = data)
cat("Model 1 Coefficients:\n")
print(coef(model_1))
# Fit Model 2: Polynomial Model y = θ0 + θ1*x1^2 + θ2*x3^2
model_2 <- lm(y ~ I(x1^2) + I(x3^2), data = data)
cat("Model 2 Coefficients:\n")
print(coef(model_2))
# Fit Model 3: Polynomial Model y = θ0 + θ1*x1^3 + θ2*x3^3
model_3 <- lm(y ~ I(x1^3) + I(x3^3), data = data)
cat("Model 3 Coefficients:\n")
print(coef(model_3))
# Fit Model 4: Polynomial Model y = θ0 + θ1*x1 + θ2*x3 + θ3*x4^2
model_4 <- lm(y ~ x1 + x3 + I(x4^2), data = data)
cat("Model 4 Coefficients:\n")
print(coef(model_4))
# Fit Model 5: Polynomial Model y = θ0 + θ1*x1 + θ2*x3 + θ3*x4^2 + θ4*x5^3
model_5 <- lm(y ~ x1 + x3 + I(x4^2) + I(x5^3), data = data)
cat("Model 5 Coefficients:\n")
print(coef(model_5))
# Residual Sum of Squares (RSS) for each model
RSS_1 <- sum(residuals(model_1)^2)
RSS_2 <- sum(residuals(model_2)^2)
RSS_3 <- sum(residuals(model_3)^2)
RSS_4 <- sum(residuals(model_4)^2)
RSS_5 <- sum(residuals(model_5)^2)
cat("RSS for Model 1:", RSS_1, "\n")
cat("RSS for Model 2:", RSS_2, "\n")
cat("RSS for Model 3:", RSS_3, "\n")
cat("RSS for Model 4:", RSS_4, "\n")
cat("RSS for Model 5:", RSS_5, "\n")
# Assuming Model 1 is selected based on the smallest RSS
model_selected <- model_1  # Change this based on task requirements
# Extract coefficients
theta_hat <- coef(model_selected)
# Print the coefficients
cat("Estimated coefficients for the selected model:\n")
print(theta_hat)
# Select two parameters with the largest absolute value from coefficients
param_values <- abs(theta_hat[-1])  # Exclude intercept
param_names <- names(theta_hat)[-1]  # Exclude intercept
largest_params <- sort(param_values, decreasing = TRUE)[1:2]
selected_params <- param_names[param_values %in% largest_params]
selected_theta_hat <- theta_hat[selected_params]
cat("Selected Parameters and their Estimates:\n")
print(selected_theta_hat)
# Step 1: Define uniform prior ranges
# Use a range of ±10% of the estimated coefficients as prior
prior_ranges <- sapply(selected_theta_hat, function(x) abs(x) * 0.1)
# Step 2: Draw samples from the uniform prior
n_samples <- 10000
samples <- matrix(NA, nrow = n_samples, ncol = length(selected_params))
colnames(samples) <- selected_params
set.seed(123)  # For reproducibility
for (i in 1:n_samples) {
samples[i, ] <- runif(length(selected_params),
min = selected_theta_hat - prior_ranges,
max = selected_theta_hat + prior_ranges)
}
# Step 3: Rejection ABC - Define the distance metric (squared error between predicted and observed values)
abc_distance <- function(sample, model, X, y) {
model$coefficients[selected_params] <- sample
predictions <- predict(model, newdata = X)
sum((predictions - y)^2)
}
# Step 4: Perform Rejection ABC
accepted_samples <- list()
threshold <- 1000  # Acceptable distance threshold (you may adjust based on the dataset)
for (i in 1:n_samples) {
sample <- samples[i, ]
dist <- abc_distance(sample, model_selected, X, y)
if (dist < threshold) {
accepted_samples <- append(accepted_samples, list(sample))
}
}
# Convert accepted samples to a data frame
accepted_samples_df <- do.call(rbind, accepted_samples)
colnames(accepted_samples_df) <- selected_params
# Step 5: Save plots into PDF
pdf("ABC_Posteriors.pdf", width = 12, height = 8)
# Joint posterior distribution
ggplot(data = as.data.frame(accepted_samples_df), aes(x = selected_params[1], y = selected_params[2])) +
geom_point(alpha = 0.5) +
ggtitle("Joint Posterior Distribution for Parameters") +
xlab(selected_params[1]) + ylab(selected_params[2])
# Marginal posterior distribution for the first parameter
ggplot(data = as.data.frame(accepted_samples_df), aes_string(x = selected_params[1])) +
geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black", alpha = 0.7) +
ggtitle(paste("Marginal Posterior Distribution for", selected_params[1])) +
xlab(selected_params[1]) + ylab("Frequency")
# Marginal posterior distribution for the second parameter
ggplot(data = as.data.frame(accepted_samples_df), aes_string(x = selected_params[2])) +
geom_histogram(binwidth = 0.05, fill = "salmon", color = "black", alpha = 0.7) +
ggtitle(paste("Marginal Posterior Distribution for", selected_params[2])) +
xlab(selected_params[2]) + ylab("Frequency")
# Close the PDF device
dev.off()
# Load necessary libraries
library(ggplot2)
library(MASS)
# Load dataset (make sure the path is correct)
data <- read.csv("E:/Intro to Stats/R Task3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Load necessary libraries
library(ggplot2)
library(MASS)
# Load dataset (make sure the path is correct)
data <- read.csv("E:\R Task\R Task 3\data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Load necessary libraries
library(ggplot2)
library(MASS)
# Load dataset (make sure the path is correct)
data <- read.csv("E:/R Task/R Task 3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Ensure numeric data
data <- data.frame(lapply(data, function(col) if(is.factor(col) || is.character(col)) as.numeric(as.character(col)) else col))
# Split data into 70% training and 30% testing
set.seed(123)
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define the selected 'best' model
best_model <- lm(x2 ~ I(x4) + I(x1^3) + I(x3^4), data = train_data)
# Get the model coefficients and their absolute values
model_coeffs <- summary(best_model)$coefficients[, 1]  # Estimated coefficients
abs_model_coeffs <- abs(model_coeffs)
# Identify the two parameters with the largest absolute values
sorted_coeffs <- sort(abs_model_coeffs, decreasing = TRUE)
top_two_params <- names(sorted_coeffs)[1:2]
# Get the values of those parameters from the model
param_values <- model_coeffs[top_two_params]
# Define prior ranges based on model coefficients (increase range for uniform prior)
param_ranges <- data.frame(
param = top_two_params,
lower = param_values - 0.5 * abs(param_values),  # Increased range for uniform prior by 50%
upper = param_values + 0.5 * abs(param_values)   # 50% of the coefficient value for prior range
)
# Rejection ABC
n_samples <- 1000
accepted_samples <- data.frame(param1 = numeric(), param2 = numeric())
tolerance <- 10000  # Further increased tolerance for rejection ABC (e.g., 10000)
for (i in 1:n_samples) {
# Sample from uniform priors
param1_sample <- runif(1, param_ranges$lower[1], param_ranges$upper[1])
param2_sample <- runif(1, param_ranges$lower[2], param_ranges$upper[2])
# Fix all other parameters in the model (based on the estimated values)
fixed_params <- model_coeffs
fixed_params[top_two_params] <- c(param1_sample, param2_sample)
# Calculate predicted values from the model using sampled parameters
y_pred <- fixed_params[1] * (test_data$x4) + fixed_params[2] * (test_data$x1^3) + fixed_params[3] * (test_data$x3^4)
# Compute residuals (difference between predicted and actual values)
residuals <- test_data$x2 - y_pred
sse <- sum(residuals^2)  # Sum of squared errors
# Print the SSE for debugging
if (i %% 100 == 0) {  # Print every 100 iterations for debugging
cat("Iteration:", i, "SSE:", sse, "\n")
}
# Rejection step: Accept if SSE is within tolerance
if (sse < tolerance) {
accepted_samples <- rbind(accepted_samples, data.frame(param1 = param1_sample, param2 = param2_sample))
}
}
# Check if any samples were accepted
if (nrow(accepted_samples) == 0) {
print("No samples accepted. Try adjusting the tolerance or prior ranges.")
} else {
# Plot the joint and marginal posterior distributions
# Joint Posterior Distribution
ggplot(accepted_samples, aes(x = param1, y = param2)) +
geom_point(alpha = 0.5, color = "blue") +
labs(title = "Joint Posterior Distribution of the Two Parameters", x = "Parameter 1", y = "Parameter 2") +
theme_minimal()
# Marginal Posterior Distributions for Parameter 1
ggplot(accepted_samples, aes(x = param1)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.5) +
labs(title = "Marginal Posterior Distribution of Parameter 1", x = "Parameter 1", y = "Frequency") +
theme_minimal()
# Marginal Posterior Distributions for Parameter 2
ggplot(accepted_samples, aes(x = param2)) +
geom_histogram(bins = 30, fill = "red", alpha = 0.5) +
labs(title = "Marginal Posterior Distribution of Parameter 2", x = "Parameter 2", y = "Frequency") +
theme_minimal()
}
# Load necessary libraries
library(ggplot2)
library(MASS)
# Load dataset
data <- read.csv("E:/R Task/R Task 3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Ensure numeric data
data <- data.frame(lapply(data, function(col) if (is.factor(col) || is.character(col)) as.numeric(as.character(col)) else col))
# Split data into 70% training and 30% testing
set.seed(123)
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define the selected 'best' model
best_model <- lm(x2 ~ I(x4) + I(x1^3) + I(x3^4), data = train_data)
# Get the model coefficients and their absolute values
model_coeffs <- summary(best_model)$coefficients[, 1]  # Estimated coefficients
abs_model_coeffs <- abs(model_coeffs)
# Identify the two parameters with the largest absolute values
sorted_coeffs <- sort(abs_model_coeffs, decreasing = TRUE)
top_two_params <- names(sorted_coeffs)[1:2]
# Get the values of those parameters from the model
param_values <- model_coeffs[top_two_params]
# Define prior ranges based on model coefficients
param_ranges <- data.frame(
param = top_two_params,
lower = param_values - 0.5 * abs(param_values),
upper = param_values + 0.5 * abs(param_values)
)
# Rejection ABC
n_samples <- 1000
accepted_samples <- data.frame(param1 = numeric(), param2 = numeric())
tolerance <- 10000  # Increased tolerance for rejection ABC
for (i in 1:n_samples) {
# Sample from uniform priors
param1_sample <- runif(1, param_ranges$lower[1], param_ranges$upper[1])
param2_sample <- runif(1, param_ranges$lower[2], param_ranges$upper[2])
# Fix all other parameters in the model
fixed_params <- model_coeffs
fixed_params[top_two_params] <- c(param1_sample, param2_sample)
# Calculate predicted values using sampled parameters
y_pred <- fixed_params[1] * test_data$x4 + fixed_params[2] * (test_data$x1^3) + fixed_params[3] * (test_data$x3^4)
# Compute residuals and SSE
residuals <- test_data$x2 - y_pred
sse <- sum(residuals^2)
# Rejection step
if (sse < tolerance) {
accepted_samples <- rbind(accepted_samples, data.frame(param1 = param1_sample, param2 = param2_sample))
}
}
# Save plots in a PDF
pdf("ABC_Posteriors.pdf", width = 10, height = 8)
# Joint Posterior Distribution
ggplot(accepted_samples, aes(x = param1, y = param2)) +
geom_point(alpha = 0.5, color = "blue") +
labs(title = "Joint Posterior Distribution of the Two Parameters", x = "Parameter 1", y = "Parameter 2") +
theme_minimal() +
print()
# Marginal Posterior for Parameter 1
ggplot(accepted_samples, aes(x = param1)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.5) +
labs(title = "Marginal Posterior Distribution of Parameter 1", x = "Parameter 1", y = "Frequency") +
theme_minimal() +
print()
# Load necessary libraries
library(ggplot2)
library(MASS)
# Load dataset
data <- read.csv("E:/R Task/R Task 3/data_03cf4b0d-8941-4323-998c-a7ff1a83d0f2_1733812247131.csv")
# Ensure numeric data
data <- data.frame(lapply(data, function(col) if (is.factor(col) || is.character(col)) as.numeric(as.character(col)) else col))
# Split data into 70% training and 30% testing
set.seed(123)
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define the selected 'best' model
best_model <- lm(x2 ~ I(x4) + I(x1^3) + I(x3^4), data = train_data)
# Get the model coefficients and their absolute values
model_coeffs <- summary(best_model)$coefficients[, 1]  # Estimated coefficients
abs_model_coeffs <- abs(model_coeffs)
# Identify the two parameters with the largest absolute values
sorted_coeffs <- sort(abs_model_coeffs, decreasing = TRUE)
top_two_params <- names(sorted_coeffs)[1:2]
# Get the values of those parameters from the model
param_values <- model_coeffs[top_two_params]
# Define prior ranges based on model coefficients
param_ranges <- data.frame(
param = top_two_params,
lower = param_values - 0.5 * abs(param_values),
upper = param_values + 0.5 * abs(param_values)
)
# Rejection ABC
n_samples <- 1000
accepted_samples <- data.frame(param1 = numeric(), param2 = numeric())
tolerance <- 10000  # Increased tolerance for rejection ABC
for (i in 1:n_samples) {
# Sample from uniform priors
param1_sample <- runif(1, param_ranges$lower[1], param_ranges$upper[1])
param2_sample <- runif(1, param_ranges$lower[2], param_ranges$upper[2])
# Fix all other parameters in the model
fixed_params <- model_coeffs
fixed_params[top_two_params] <- c(param1_sample, param2_sample)
# Calculate predicted values using sampled parameters
y_pred <- fixed_params[1] * test_data$x4 + fixed_params[2] * (test_data$x1^3) + fixed_params[3] * (test_data$x3^4)
# Compute residuals and SSE
residuals <- test_data$x2 - y_pred
sse <- sum(residuals^2)
# Rejection step
if (sse < tolerance) {
accepted_samples <- rbind(accepted_samples, data.frame(param1 = param1_sample, param2 = param2_sample))
}
}
# Save plots in a PDF
pdf("ABC_Posteriors.pdf", width = 10, height = 8)
# Joint Posterior Distribution
ggplot(accepted_samples, aes(x = param1, y = param2)) +
geom_point(alpha = 0.5, color = "blue") +
labs(title = "Joint Posterior Distribution of the Two Parameters", x = "Parameter 1", y = "Parameter 2") +
theme_minimal()
# Marginal Posterior for Parameter 1
ggplot(accepted_samples, aes(x = param1)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.5) +
labs(title = "Marginal Posterior Distribution of Parameter 1", x = "Parameter 1", y = "Frequency") +
theme_minimal()
# Marginal Posterior for Parameter 2
ggplot(accepted_samples, aes(x = param2)) +
geom_histogram(bins = 30, fill = "red", alpha = 0.5) +
labs(title = "Marginal Posterior Distribution of Parameter 2", x = "Parameter 2", y = "Frequency") +
theme_minimal()
dev.off()
